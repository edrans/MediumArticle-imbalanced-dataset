{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4401260d",
   "metadata": {},
   "source": [
    "# Install Required Packages via PiP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b45023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker pandas numpy imbalanced-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53069e01",
   "metadata": {},
   "source": [
    "# Set up SageMaker Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afe8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker \n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = \"rzoghbi-medium-smote-article-dataset\" # Replace with the bucket where your Data is located.\n",
    "subfolder = \"\" # Prefix \n",
    "\n",
    "\n",
    "\n",
    "# Define IAM role\n",
    "import boto3 # AWS Python SDK\n",
    "from sagemaker import get_execution_role #Defined when you create your instance\n",
    "import os\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Note: The execution role is only available when running a notebook within SageMaker. \n",
    "# If you run get_execution_role in a notebook not on SageMaker, expect a region error.\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7aa4b",
   "metadata": {},
   "source": [
    "# Import Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75532b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as sts\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22cbbb",
   "metadata": {},
   "source": [
    "### Test connection with S3 Bucket, if properly setup, you should see contents of the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffec23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = boto3.client('s3')\n",
    "contents = conn.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "for f in contents:\n",
    "    print(f['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43657c9d",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e64443",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51572c0e",
   "metadata": {},
   "source": [
    "Raw dataset is located on the root of S3 Bucket, we'll import it as a Pandas Dataframe and transform it into a ML suited dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d8328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_key = 'dataset.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, sep=',')\n",
    "\n",
    "# Take a look at the DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed952d",
   "metadata": {},
   "source": [
    "### Check for Null/Empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f95b0",
   "metadata": {},
   "source": [
    "Only \"BMI\" and \"smoking_status\" have Null Values, we will deal with them later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20e205",
   "metadata": {},
   "source": [
    "Let's first encode the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac771fd",
   "metadata": {},
   "source": [
    "### Drop Irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd8237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop ID as it is meaningless for the ML Model\n",
    "df.drop(['id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13485c82",
   "metadata": {},
   "source": [
    "### Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.replace({'gender':{'Female':0, 'Male':1, 'Other':2}}, inplace=True)\n",
    "df.replace({'ever_married':{'No':0, 'Yes':1}}, inplace=True)\n",
    "df.replace({'Residence_type':{'Urban':0, 'Rural':1}}, inplace=True)\n",
    "df.replace({'work_type':{'Private':0, 'Self-employed':1, 'children':2, 'Govt_job':3, 'Never_worked':4}}, inplace=True)\n",
    "df.replace({'smoking_status':{'never smoked':0, 'formerly smoked':1, 'smokes':2}}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e87e02",
   "metadata": {},
   "source": [
    "#### Encode age in bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05320fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.age = df.age.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df.loc[ df['age'] <= 16, 'age'] = 0\n",
    "df.loc[(df['age'] > 16) & (df['age'] <= 32), 'age'] = 1\n",
    "df.loc[(df['age'] > 32) & (df['age'] <= 48), 'age'] = 2\n",
    "df.loc[(df['age'] > 48) & (df['age'] <= 64), 'age'] = 3\n",
    "df.loc[ df['age'] > 64, 'age'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c7012",
   "metadata": {},
   "source": [
    "#### Review encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eaafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ac3b0",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa413d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a0ba7",
   "metadata": {},
   "source": [
    "\"BMI\" has 1462 null values --- \n",
    "\"smoking_status\" has 13292 null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8279ed",
   "metadata": {},
   "source": [
    "#### To Fill body mass index Null data we will assume a normal distribution. We will use the mean value and, from there, we will calculate the standard deviation.\n",
    "#### Then, we will fill NaN with a random value from (mean - standard deviation) to (mean + standard deviation)\n",
    "#### Assuming a normal distribution, majority of the data should fall in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d458b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statistics as sts\n",
    "from random import randrange, uniform\n",
    "\n",
    "# Remove NaNs, place values in a new temporary series\n",
    "bmi_raw = df.bmi.dropna()\n",
    "\n",
    "# Mean value\n",
    "raw_bmi_avg = sts.mean(bmi_raw)\n",
    "# Standard Deviation\n",
    "raw_bmi_stdev = sts.mean(bmi_raw)\n",
    "# Lower possibe value for BMI\n",
    "lower_treshold = raw_bmi_avg - raw_bmi_stdev\n",
    "# Higher possible value for BMI\n",
    "upper_treshold = raw_bmi_avg + raw_bmi_stdev\n",
    "\n",
    "# Input random value to fill NaN\n",
    "df.bmi.fillna(uniform(lower_treshold,upper_treshold) , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bmi.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537df24b",
   "metadata": {},
   "source": [
    "#### To fill NaN in smoking_status, we will input a random value between 0 and 3\n",
    "#### This may not be the best practice, but it's fine for our use case. Not for a real problem solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.smoking_status.fillna(randrange(0,3), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fa2fe",
   "metadata": {},
   "source": [
    "#### As you can see, there are no more Null Values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aba6e",
   "metadata": {},
   "source": [
    "#### Let's take a glance at how the dataset is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcfb56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e85864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c9a99",
   "metadata": {},
   "source": [
    "#### Let's apply standarization to our numerical features 'avg_glucose_level' and 'bmi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a311c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toscale = df[['avg_glucose_level' , 'bmi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55189bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toscale.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "trans = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized = trans.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ff86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "scaled = DataFrame(standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.columns = ['avg_glucose_level' , 'bmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.avg_glucose_level = scaled.avg_glucose_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bmi = scaled.bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45b42a",
   "metadata": {},
   "source": [
    "### Exporting transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d41772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf96388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f952339",
   "metadata": {},
   "source": [
    "#### Now that we have tranformed the dataset, we want to save it to file and export it to S3. So, in case we need to open a new work, we may not have to make all pre-processing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save this pre processed dataset. For this, copy the dataset into a new object\n",
    "transformed_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00aa74",
   "metadata": {},
   "source": [
    "#### We will use XGboost, so we need an extra step: To place the label on the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost requires label on the first column, let's rotate last column to first\n",
    "\n",
    "cols = list(transformed_df.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "transformed_df = transformed_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d280982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review final state\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6af869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV the pre processed dataset\n",
    "transformed_df.to_csv(\"transformed.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a34c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(subfolder, \"transformed.csv\")\n",
    ").upload_file(\"transformed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0f926",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c34c8d",
   "metadata": {},
   "source": [
    "### Review label class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eac505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_df.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235fa89",
   "metadata": {},
   "source": [
    "#### There are almost 800 positive cases per over 42 thousand negatives. This is a severe skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9a30a",
   "metadata": {},
   "source": [
    "### Let's graph and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0507f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.stroke.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035c28f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14170688",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85e7fd",
   "metadata": {},
   "source": [
    "## As a last step in this notebook, we will split our dataset and resample to overcome the imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78fde2",
   "metadata": {},
   "source": [
    "### We will prepare four pairs of train-validation datasets and one pair for test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919734a2",
   "metadata": {},
   "source": [
    "#### Our four pairs of train-validation datasets are:\n",
    "##### train-validation (imbalanced, original dataset)\n",
    "##### train-validation SMOTE \n",
    "##### train-validation KMeans SMOTE\n",
    "##### train-validation SVM-SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b14aa4",
   "metadata": {},
   "source": [
    "##### Test dataset will remain untouched as it will be the same for the evaluation of the training procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b828f5f",
   "metadata": {},
   "source": [
    "###   Remember!!   Resampling should be done on the training dataset only, so we will split our dataset and then perform our imbalance remediation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c1bd1",
   "metadata": {},
   "source": [
    "#### First Split: We will split the dataset into a train-validation dataset and a test dataset. Test dataset will remain unchanged, while train-validation will be further split into separate train and validation datasets. First split: Train-Validation 80% - Test 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split for Initial training. Train-Test Split should be donve before resample\n",
    "train_validation , test = train_test_split(transformed_df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83f3da",
   "metadata": {},
   "source": [
    "#### Second split: We split into train and validation because XGBoost requires separate datasets for train and validate\n",
    "#### Second split: train 70% validation 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ba4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation split before SMOTE\n",
    "train , validation = train_test_split (train_validation, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724c446",
   "metadata": {},
   "source": [
    "#### Train, Validation and Test are saved in CSV and uploaded to S3 to be called by our training object.\n",
    "#### Also, train-validation is saved into CSV and uploaded for future references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation.to_csv(\"validation.csv\", header=False, index=False)\n",
    "test.to_csv(\"test.csv\", header=False, index=False)\n",
    "train_validation.to_csv(\"train_validation.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124eada",
   "metadata": {},
   "source": [
    "### We upload our splitted dataset to S3, as our model will reference them in the future for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train\", \"train.csv\")\n",
    ").upload_file(\"train.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"validation\", \"validation.csv\")\n",
    ").upload_file(\"validation.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"test\", \"test.csv\")\n",
    ").upload_file(\"test.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train_validation\", \"train_validation.csv\")\n",
    ").upload_file(\"train_validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ebc7c",
   "metadata": {},
   "source": [
    "### Also, we are keeping a second copy of the test dataset, without labels, to make future batch_inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test.drop(['stroke'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a517197",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch.to_csv(\"test_batch.csv\", header=False, index=False)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"test_batch\", \"test_batch.csv\")\n",
    ").upload_file(\"test_batch.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96106038",
   "metadata": {},
   "source": [
    "### Now we have our train-validation dataset withour resampling. Also, we already have our test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef70ec",
   "metadata": {},
   "source": [
    "## SMOTE Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0dbb7",
   "metadata": {},
   "source": [
    "### SMOTE library requires separate datasets for data and labels, so we will take our train_validation dataset we got on previous step and from there, resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9c9ea",
   "metadata": {},
   "source": [
    "#### Retrieve train_validation dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'train_validation.csv'\n",
    "subfolder = \"train_validation\"\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, subfolder, data_key)\n",
    "\n",
    "df2 = pd.read_csv(data_location, sep=',', header= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255b00c",
   "metadata": {},
   "source": [
    "Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8325769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['stroke', 'gender', 'age', 'hypertension', 'heart_disease',\n",
    "       'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level',\n",
    "       'bmi', 'smoking_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11659e1d",
   "metadata": {},
   "source": [
    "#### Split into data and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a03446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df2.iloc[: , 1:11]\n",
    "y_train = df2.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7479917",
   "metadata": {},
   "source": [
    "#### Resample dataset using default smote. Input: X_train (data), y_train (labels). Output : X_train_smote (resampled data), y_train_smote (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_oversampled = 17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE (sampling_strategy = {1 : positive_oversampled} , random_state = 42)\n",
    "\n",
    "# Resample dataset \n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750909c",
   "metadata": {},
   "source": [
    "#### Reassamble dataset, as XGBoost requires labels in first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataset called XY_train_smote, where we will merge our data and labels\n",
    "XY_train_smote = X_train_smote\n",
    "XY_train_smote['stroke'] = y_train_smote\n",
    "\n",
    "# Put labels on first column\n",
    "cols = list(XY_train_smote.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "XY_train_smote = XY_train_smote[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302372b",
   "metadata": {},
   "source": [
    "#### How are class distributed after resample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cef150",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_smote.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_smote.stroke.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01c37f",
   "metadata": {},
   "source": [
    "#### Minority class is oversampled until equalize the majority class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0cd09a",
   "metadata": {},
   "source": [
    "### Train-Validation Split: XGBoost requieres separate dataset for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4dbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smote, validation_smote = train_test_split(XY_train_smote, test_size = 0.3, random_state =42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b1c46",
   "metadata": {},
   "source": [
    "#### We will upload the Datasets to S3 to make them available for our Training Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smote.to_csv(\"train_smote.csv\", header=False, index=False)\n",
    "validation_smote.to_csv(\"validation_smote.csv\", header=False, index=False)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train_smote\", \"train_smote.csv\")\n",
    ").upload_file(\"train_smote.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"validation_smote\", \"validation_smote.csv\")\n",
    ").upload_file(\"validation_smote.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b1e23",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5d45a",
   "metadata": {},
   "source": [
    "## KMeans SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "sm = KMeansSMOTE(random_state=42, sampling_strategy = {1 : positive_oversampled})\n",
    "sm.set_params(**{'cluster_balance_threshold' : 0.05 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_KMSmote, y_train_KMSmote = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataset called XY_train_smote, where we will merge our data and labels\n",
    "XY_train_KMSmote = X_train_KMSmote\n",
    "XY_train_KMSmote['stroke'] = y_train_KMSmote\n",
    "\n",
    "# Put labels on first column\n",
    "cols = list(XY_train_KMSmote.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "XY_train_KMSmote = XY_train_KMSmote[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdfe26",
   "metadata": {},
   "source": [
    "#### How are class distributed after resample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_KMSmote.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ab544",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_KMSmote.stroke.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c52f",
   "metadata": {},
   "source": [
    "#### Minority class is oversampled until equalize the majority class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55992cfd",
   "metadata": {},
   "source": [
    "### Train-Validation Split: XGBoost requieres separate dataset for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_KMSmote, validation_KMSmote = train_test_split(XY_train_KMSmote, test_size = 0.3, random_state =42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d287d97",
   "metadata": {},
   "source": [
    "#### We will upload the Datasets to S3 to make them available for our Training Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_KMSmote.to_csv(\"train_KMSmote.csv\", header=False, index=False)\n",
    "validation_KMSmote.to_csv(\"validation_KMSmote.csv\", header=False, index=False)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train_KMSmote\", \"train_KMSmote.csv\")\n",
    ").upload_file(\"train_KMSmote.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"validation_KMSmote\", \"validation_KMSmote.csv\")\n",
    ").upload_file(\"validation_KMSmote.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c4b8c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf935f",
   "metadata": {},
   "source": [
    "## SVM SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SVMSMOTE\n",
    "sm = SVMSMOTE(sampling_strategy = {1 : positive_oversampled}, random_state=42, k_neighbors = 5, m_neighbors = 10, out_step = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm, y_train_svm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad454a",
   "metadata": {},
   "source": [
    "#### Reassamble dataset, as XGBoost requires labels in first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataset called XY_train_smote, where we will merge our data and labels\n",
    "XY_train_svm = X_train_svm\n",
    "XY_train_svm['stroke'] = y_train_svm\n",
    "\n",
    "# Put labels on first column\n",
    "cols = list(XY_train_svm.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "XY_train_svm = XY_train_svm[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2e0a8",
   "metadata": {},
   "source": [
    "#### How are class distributed after resample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca340711",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_svm.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cda49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_svm.stroke.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e17432",
   "metadata": {},
   "source": [
    "##### For SVM SMOTE, minority class is oversampled, however not equalized to majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676c65c",
   "metadata": {},
   "source": [
    "### Train-Validation Split: XGBoost requieres separate dataset for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm, validation_svm = train_test_split(XY_train_svm, test_size = 0.3, random_state =42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69cafe",
   "metadata": {},
   "source": [
    "#### We will upload the Datasets to S3 to make them available for our Training Instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm.to_csv(\"train_svm.csv\", header=False, index=False)\n",
    "validation_svm.to_csv(\"validation_svm.csv\", header=False, index=False)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train_svm\", \"train_svm.csv\")\n",
    ").upload_file(\"train_svm.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"validation_svm\", \"validation_svm.csv\")\n",
    ").upload_file(\"validation_svm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a8e2e",
   "metadata": {},
   "source": [
    "## SMOTE ENN Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b799149",
   "metadata": {},
   "source": [
    "#### SMOTE ENN implements a combination of oversampling minority class and undersampling majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0, sampling_strategy = {1 : positive_oversampled})\n",
    "X_train_smteenn, y_train_smteenn = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smteenn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd304857",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_SmoteENN = X_train_smteenn\n",
    "XY_train_SmoteENN['stroke'] = y_train_smteenn\n",
    "\n",
    "# Put labels on first column\n",
    "cols = list(XY_train_SmoteENN.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "XY_train_SmoteENN = XY_train_SmoteENN[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_SmoteENN.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87439d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_SmoteENN.stroke.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SmoteENN, validation_SmoteENN = train_test_split(XY_train_SmoteENN, test_size = 0.3, random_state =42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SmoteENN.to_csv(\"train_SmoteENN.csv\", header=False, index=False)\n",
    "validation_SmoteENN.to_csv(\"validation_SmoteENN.csv\", header=False, index=False)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"train_SmoteENN\", \"train_SmoteENN.csv\")\n",
    ").upload_file(\"train_SmoteENN.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(\"validation_SmoteENN\", \"validation_SmoteENN.csv\")\n",
    ").upload_file(\"validation_SmoteENN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d5936",
   "metadata": {},
   "source": [
    "## Now that we have our dataset processed and splitted, we will go ahead and train our model and evaluate the effects of the oversampling techniques applied in previous sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3818f",
   "metadata": {},
   "source": [
    "### Please refer to ACV Model Training to proceed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
